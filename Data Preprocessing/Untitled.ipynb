{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86df71b-9fb9-4677-8099-87e26b92d3f6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# âœ… **Complete Guide to Data Preprocessing in Machine Learning**  \n",
    "\n",
    "Preprocessing is a **crucial step** in machine learning that can **make or break your modelâ€™s performance**. Different models **handle data differently**, so applying the correct techniques **saves time** and **improves accuracy**.  \n",
    "\n",
    "Letâ€™s go **step by step** ğŸ‘‡  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ **1. Understand the Data Type First**  \n",
    "Before any transformation, **identify the types of data** in your dataset.  \n",
    "\n",
    "### ğŸ”¹ **Types of Data**  \n",
    "| **Data Type**      | **Example**             | **Encoding Needed?** | **Scaling Needed?** |\n",
    "|--------------------|------------------------|----------------------|---------------------|\n",
    "| **Numerical (Continuous)** | Age, Salary, Temperature | âŒ No | âœ… Yes |\n",
    "| **Numerical (Discrete)** | Number of Children, Counts | âŒ No | âœ… Yes |\n",
    "| **Categorical (Nominal)** | Gender, City, Country | âœ… Yes (One-Hot, Label) | âŒ No |\n",
    "| **Categorical (Ordinal)** | Low-Medium-High, Education Level | âœ… Yes (Label Encoding) | âœ… Sometimes |\n",
    "\n",
    "ğŸ”¹ **Tools:**  \n",
    "```python\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe(include='all')\n",
    "df.dtypes\n",
    "```\n",
    "\n",
    "ğŸ‘‰ **Why is this important?**  \n",
    "- **Some models (like Decision Trees, Random Forests, XGBoost) can handle categorical variables directly** without encoding!  \n",
    "- **Linear models (like Logistic Regression, SVM) need numerical input**, so categorical features **must be encoded**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ **2. Handling Missing Values**  \n",
    "### ğŸ”¹ **Rules of Thumb**  \n",
    "âœ” **If missing values < 5%** â†’ Drop rows (`df.dropna()`).  \n",
    "âœ” **If missing values 5-40%** â†’ Impute using:  \n",
    "   - **Numerical** â†’ Mean (if normal), Median (if skewed), KNN Imputer.  \n",
    "   - **Categorical** â†’ Mode (most frequent value) or \"Unknown\".  \n",
    "âœ” **If missing values > 40%** â†’ Drop column if it's not important.  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```python\n",
    "# Check for missing values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# Check total number of missing values\n",
    "df.isnull().sum().sum()\n",
    "\n",
    "# Visualize missing values (optional, using seaborn)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# drop rows with missing values\n",
    "df_dropped_rows = df.dropna()\n",
    "\n",
    "# drop columns with missing values\n",
    "df_dropped_columns = df.dropna(axis=1)\n",
    "\n",
    "# Replace missing values with zero\n",
    "df_filled = df.fillna(0)\n",
    "\n",
    "# Numeric columns mean or median imputaion\n",
    "df['numeric_column'] = df['numeric_column'].fillna(df['numeric_column'].mean())\n",
    "df['numeric_column'] = df['numeric_column'].fillna(df['numeric_column'].median())\n",
    "\n",
    "# Categorical columns mode imputation\n",
    "df['categorical_column'] = df['categorical_column'].fillna(df['categorical_column'].mode()[0])\n",
    "\n",
    "# backward or forward fill\n",
    "# Forward fill (propagate the last valid value forward)\n",
    "df_filled_forward = df.fillna(method='ffill')\n",
    "\n",
    "# Backward fill (propagate the next valid value backward)\n",
    "df_filled_backward = df.fillna(method='bfill')\n",
    "\n",
    "# Impute using scikit learn - using simpleimputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For numeric data\n",
    "imputer = SimpleImputer(strategy='mean')  # Can use 'median', 'most_frequent', or 'constant'\n",
    "df[['numeric_column']] = imputer.fit_transform(df[['numeric_column']])\n",
    "\n",
    "# For categorical data\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['categorical_column']] = imputer.fit_transform(df[['categorical_column']])\n",
    "\n",
    "# for time series data - use interpolation\n",
    "# Linear interpolation-- Use linear when data points are equally spaced (e.g., daily data).\n",
    "df_interpolated = df.interpolate(method='linear')\n",
    "\n",
    "# Time-based interpolation (if the index is datetime) --Use time for unevenly spaced time-series data (e.g., hourly data with missing records).\n",
    "\n",
    "df_time_interpolated = df.interpolate(method='time')\n",
    "\n",
    "```\n",
    "\n",
    "ğŸ‘‰ **Which ML models handle missing values?**  \n",
    "| Model | Needs Missing Value Handling? |\n",
    "|--------|-----------------------------|\n",
    "| Decision Trees, Random Forest | âŒ No, can handle missing values |\n",
    "| XGBoost | âœ… Yes, but can handle some missing values |\n",
    "| Logistic Regression, SVM, KNN | âœ… Yes, requires imputation |\n",
    "| Neural Networks | âœ… Yes, missing values must be handled |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ **3. Encoding Categorical Data**  \n",
    "### ğŸ”¹ **Rules of Thumb**  \n",
    "âœ” **Few unique values (<10 categories):** **One-Hot Encoding** (`pd.get_dummies()`).  \n",
    "âœ” **Many unique values (>10 categories):** **Target Encoding** (for tree-based models).  \n",
    "âœ” **Ordinal data (e.g., Low-Medium-High):** **Label Encoding**.  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```python\n",
    "# 1.One-Hot Encoding\n",
    "\n",
    "# Converts each category into a separate binary column (0 or 1).\n",
    "df = pd.get_dummies(df, columns=['category_column'], drop_first=True)\n",
    "or\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(df[['Color']])\n",
    "# 2.Label Encoding\n",
    "\n",
    "# Converts each unique category into an integer.\n",
    "# Suitable for ordinal data (where the order matters, e.g., Low, Medium, High).\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['ordinal_column'] = le.fit_transform(df['ordinal_column'])\n",
    "\n",
    "# 3.Ordinal Encoding\n",
    "\n",
    "# For ordered categories, you can specify the rank manually.\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "data = [['Low'], ['Medium'], ['High']]\n",
    "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
    "encoded = encoder.fit_transform(data)\n",
    "print(encoded)  # Output: [[0], [1], [2]]\n",
    "\n",
    "```\n",
    "\n",
    "ğŸ‘‰ **Which ML models require encoding?**  \n",
    "| Model | Needs Categorical Encoding? |\n",
    "|--------|-----------------------------|\n",
    "| Decision Trees, Random Forest | âŒ No, can handle categorical data directly |\n",
    "| XGBoost, LightGBM | âœ… Yes, but can handle some categorical variables |\n",
    "| Logistic Regression, SVM, KNN | âœ… Yes, needs numerical encoding |\n",
    "| Neural Networks | âœ… Yes, needs numerical encoding |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ **4. Handling Outliers**  \n",
    "### ğŸ”¹ **Rules of Thumb**  \n",
    "âœ” **For skewed numerical data**, use **log transformation** (`np.log1p()`).  \n",
    "âœ” **For extreme outliers**, use **Winsorization (capping)** or remove using **IQR method**.  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```python\n",
    "\n",
    "ğŸ›  Methods to Detect Outliers\n",
    "\n",
    " 1. Z score method\n",
    "Detects how many standard deviations a data point is from the mean.\n",
    "\n",
    "Rule:\n",
    "If the Z-score > 3 or < -3 â†’ It's an outlier.\n",
    "\n",
    "Code:\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "data = np.array([10, 12, 15, 14, 13, 120, 14, 13, 12])\n",
    "\n",
    "z_scores = zscore(data)\n",
    "\n",
    "outliers = np.where(np.abs(z_scores) > 3)\n",
    "print(\"Outliers:\", data[outliers])\n",
    "\n",
    "\n",
    "2. IQR Method\n",
    "Detects outliers based on the range between the 25th percentile (Q1) and 75th percentile (Q3).\n",
    "\n",
    "Rule:\n",
    "\n",
    "Code:\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({'Values': [10, 12, 15, 14, 13, 120, 14, 13, 12]})\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = df['Values'].quantile(0.25)\n",
    "Q3 = df['Values'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['Values'] < (Q1 - 1.5 * IQR)) | (df['Values'] > (Q3 + 1.5 * IQR))]\n",
    "print(\"Outliers:\\n\", outliers)\n",
    "\n",
    "3. Visualization-Based Detection\n",
    "\n",
    "Box Plot: Quickly spots outliers using IQR.\n",
    "Scatter Plot: Helps to detect outliers in two-dimensional data.\n",
    "Histogram: Reveals unusual frequency distribution\n",
    "\n",
    "\n",
    "ğŸ›  Methods to Handle Outliers\n",
    "\n",
    "ğŸ”„ 1ï¸âƒ£ Remove Outliers\n",
    "# Remove outliers using IQR\n",
    "df_no_outliers = df[(df['Values'] >= (Q1 - 1.5 * IQR)) & (df['Values'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "ğŸ”„ 2ï¸âƒ£ Cap Outliers (Winsorization)\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Apply Winsorization\n",
    "capped_data = winsorize(df['Values'], limits=[0.05, 0.05])  # Cap 5% on both ends\n",
    "\n",
    "ğŸ”„ 3ï¸âƒ£ Transform Data (Log, Square Root)\n",
    "# Log Transformation\n",
    "df['Log_Transformed'] = np.log1p(df['Values'])  # log(1 + x)\n",
    "\n",
    "ğŸ”„ 4ï¸âƒ£ Impute Outliers\n",
    "# Replace outliers with median\n",
    "median_value = df['Values'].median()\n",
    "df['Values'] = np.where(\n",
    "    (df['Values'] < (Q1 - 1.5 * IQR)) | (df['Values'] > (Q3 + 1.5 * IQR)),\n",
    "    median_value,\n",
    "    df['Values']\n",
    ")\n",
    "\n",
    "\n",
    "ğŸš© Which Method Should You Use?\n",
    "Situation :\tSuggested Method\n",
    "Outliers are data entry errors :Remove them\n",
    "Small dataset :Impute or cap outliers\n",
    "Outliers carry important information :\tTransform data (log, sqrt)\n",
    "Model is robust to outliers (tree-based):\tIgnore or handle selectively\n",
    "\n",
    "```\n",
    "\n",
    "ğŸ‘‰ **Which ML models handle outliers?**  \n",
    "| Model | Affected by Outliers? |\n",
    "|--------|----------------------|\n",
    "| Decision Trees, Random Forest | âŒ No, not sensitive |\n",
    "| XGBoost, LightGBM | âŒ No, robust to outliers |\n",
    "| Logistic Regression, SVM, KNN | âœ… Yes, need to remove or scale outliers |\n",
    "| Neural Networks | âœ… Yes, need preprocessing |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ **5. Feature Scaling**  \n",
    "\n",
    "### âœ… Why Feature Scaling?\n",
    "\n",
    "- Algorithms like **SVM, KNN, K-Means, Gradient Descent** are sensitive to feature magnitudes.\n",
    "- Speeds up convergence in optimization algorithms.\n",
    "- Prevents dominance of features with larger scales.\n",
    "- \n",
    "### ğŸ”¹ **Rules of Thumb**  \n",
    "âœ” **For normal data** â†’ **Use StandardScaler()** (Z-score normalization).  \n",
    "âœ” **For skewed data** â†’ **Use MinMaxScaler() (0-1 scaling)** or **Power Transform (Box-Cox)**.  \n",
    "âœ” **Tree-based models (Decision Trees, Random Forest, XGBoost)** **DONâ€™T need scaling**.  \n",
    "\n",
    "### ğŸ”¢ Types of Feature Scaling Techniques\n",
    "\n",
    "#### âš–ï¸ 1ï¸âƒ£ Min-Max Scaling (Normalization)\n",
    "\n",
    "**Use Case:**  \n",
    "- Scales features to a fixed range [0, 1].  \n",
    "- Useful for algorithms sensitive to magnitudes (**KNN, SVM, Neural Networks**).  \n",
    "- Not robust to outliers.\n",
    "\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[10], [20], [30], [40], [50]]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### âš–ï¸ 1ï¸âƒ£ Standardization Z score \n",
    "\n",
    "**Use Case:**  \n",
    "- Scales data to have mean = 0 and sd = 1\n",
    "- Best for algorithms assuming Gaussian distribution (Logistic Regression, Linear Regression). \n",
    "- Partially handles outliers.\n",
    "\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [[10], [20], [30], [40], [50]]\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "```\n",
    "#### Best practise : Scaling in pipeline\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Choose the appropriate scaler\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "```\n",
    "\n",
    "ğŸ‘‰ **Which ML models require scaling?**  \n",
    "| Model | Needs Scaling? |\n",
    "|--------|-------------|\n",
    "| Decision Trees, Random Forest | âŒ No |\n",
    "| XGBoost, LightGBM | âŒ No |\n",
    "| Logistic Regression, SVM, KNN | âœ… Yes, needs scaling |\n",
    "| Neural Networks | âœ… Yes, needs scaling |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ **6. Handling Imbalanced Data**  \n",
    "âœ” **For classification tasks with imbalance (>80-20 ratio):**  \n",
    "   - **Use SMOTE (Synthetic Minority Oversampling)**.  \n",
    "   - **Try undersampling if dataset is large**.  \n",
    "\n",
    "### 1ï¸âƒ£ Resampling Methods\n",
    "\n",
    "#### ğŸ”„ a) **Oversampling** (Increase minority class)\n",
    "\n",
    "- **Use Case:** When the dataset is small and losing information is unacceptable.\n",
    "- **Techniques:** \n",
    "  - Random Oversampling\n",
    "  - SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming X and y are your features and target variable\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"Resampled dataset shape:\", X_resampled.shape)\n",
    "```\n",
    "#### ğŸ”„ a) **Undersampling** (Reduce majority class)\n",
    "\n",
    "- **Use Case:** Large datasets with plenty of majority samples.\n",
    "- **Techniques:** \n",
    "  - Random Undersampling\n",
    "  - SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "print(\"Resampled dataset shape:\", X_resampled.shape)\n",
    "```\n",
    "\n",
    "\n",
    "ğŸ‘‰ **Which ML models are affected by imbalance?**  \n",
    "| Model | Affected by Imbalance? |\n",
    "|--------|----------------------|\n",
    "| Decision Trees, Random Forest | âœ… Yes, but less sensitive |\n",
    "| Logistic Regression, SVM | âœ… Yes, needs balancing |\n",
    "| Neural Networks | âœ… Yes, needs balancing |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ **7. Automating Preprocessing with Pipelines**  \n",
    "Using **Scikit-learn Pipelines** ensures **reproducibility** and saves time.  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "processed_data = pipeline.fit_transform(raw_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ **Final Summary: What You Should Do Before Using Each Model**  \n",
    "| Model | Needs Missing Value Handling? | Needs Encoding? | Needs Scaling? | Handles Outliers? |\n",
    "|--------|----------------|----------------|---------------|----------------|\n",
    "| **Decision Trees, Random Forest** | âŒ No | âŒ No | âŒ No | âœ… Yes |\n",
    "| **XGBoost, LightGBM** | âœ… Yes | âœ… Yes | âŒ No | âœ… Yes |\n",
    "| **Logistic Regression, SVM, KNN** | âœ… Yes | âœ… Yes | âœ… Yes | âŒ No |\n",
    "| **Neural Networks** | âœ… Yes | âœ… Yes | âœ… Yes | âŒ No |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be8560-e90d-4d19-af94-9321d32a14f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
